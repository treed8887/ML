% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={HW4},
  pdfauthor={Tyler Reed},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{HW4}
\author{Tyler Reed}
\date{10/27/2021}

\begin{document}
\maketitle

\hypertarget{needed-packages}{%
\subsubsection*{Needed packages}\label{needed-packages}}
\addcontentsline{toc}{subsubsection}{Needed packages}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(infer)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(matrixStats)}
\end{Highlighting}
\end{Shaded}

Working through examples in \emph{Regression and Other Stories} by
Gelman, Hill and Vehtari. Original RMD files were Solomon Kurz's
versions of examples from ROS that he originally edited but they have
been heavily edited by Professor Kapitula.

\hypertarget{bias-and-unmodeled-uncertainty}{%
\subsection{4.3 Bias and unmodeled
uncertainty}\label{bias-and-unmodeled-uncertainty}}

\begin{quote}
The inferences discussed above are all consistent on the model being
true, with unbiased measurements, random samples, and randomized
experiments. But real data collection is imperfect, and where possible
we should include the possibility of model error in our inferences and
predictions. (p.~55)
\end{quote}

\hypertarget{bias-in-estimation.}{%
\subsubsection{4.3.1 Bias in estimation.}\label{bias-in-estimation.}}

\begin{quote}
Roughly speaking, we say that an estimate is \emph{unbiased} if it is
correct on average. For a simple example, consider a survey, a simple
random sample of adults in the United States, in which each respondent
is asked the number of hours he or she spends watching television each
day. Assuming responses are complete and accurate, the average response
in the \emph{sample} is an unbiased estimate of the average number of
hours watched in the \emph{population.} (p.~55, \emph{emphasis} in the
original)
\end{quote}

If the long run average is not equal to what we are estimating we say
our estimate is biased.

Let's check that out in code. For simplicity, let's say our survey only
measures in 1-hour units and that the average number of hours spent
watching television is 2. We'll make a custom function that will take
the means of rand0m samples of the Poisson distribution of a set \(n\)
and \(\lambda\).

We will take 1,000 random samples for which \(n = 100\) and
\(\lambda = 2\). Every simulated sample is a simulated data set with a
sample size of 100.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# how many simulations would you like?}
\NormalTok{nsims }\OtherTok{\textless{}{-}} \DecValTok{1000}

\CommentTok{\# set the true data{-}generating parameters}
\NormalTok{lambda }\OtherTok{=} \DecValTok{2}
\NormalTok{n }\OtherTok{=} \DecValTok{100}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{40}\NormalTok{)}

\CommentTok{\# simulate}
\NormalTok{sims }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n}\SpecialCharTok{*}\NormalTok{nsims, }\AttributeTok{lambda =}\NormalTok{ lambda),nsims,n)}
\NormalTok{ybar }\OtherTok{\textless{}{-}} \FunctionTok{rowMeans}\NormalTok{(sims)}
\NormalTok{yMedian }\OtherTok{\textless{}{-}}\FunctionTok{rowMedians}\NormalTok{(sims)}

\NormalTok{d }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nsims,ybar,yMedian) }

\FunctionTok{glimpse}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,000
## Columns: 3
## $ i       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,~
## $ ybar    <dbl> 1.92, 2.06, 1.78, 1.88, 2.01, 2.00, 2.01, 2.08, 2.03, 2.03, 2.~
## $ yMedian <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,~
\end{verbatim}

Here is the sample distribution of our means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ybar)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }
                     \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\FloatTok{1.75}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{2.25}\NormalTok{),}
                     \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\FloatTok{1.75}\NormalTok{, }\StringTok{"2}\SpecialCharTok{\textbackslash{}n}\StringTok{(i.e., the population mean)"}\NormalTok{, }\FloatTok{2.25}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{breaks =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"The sampling distribution of the sample means"}\NormalTok{,}\AttributeTok{subtitle =} \FunctionTok{expression}\NormalTok{(}\StringTok{"The sample means are unbiased estimates of the mean for Poisson"}\SpecialCharTok{\textasciitilde{}}\NormalTok{(lambda}\SpecialCharTok{==}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=396px]{STA631-HW4-TylerReed_files/figure-latex/unnamed-chunk-4-1} \end{center}

Now formally check how we did by computing the mean of our sample of
means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(ybar))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##    mean
##   <dbl>
## 1  2.00
\end{verbatim}

Yep, we got within rounding error of the true mean, 2.

Is the median unbiased???

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{meanofMedians =} \FunctionTok{mean}\NormalTok{(yMedian))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   meanofMedians
##           <dbl>
## 1          1.97
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ yMedian)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth=}\NormalTok{.}\DecValTok{25}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }
                     \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{,}\FloatTok{1.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{2.5}\NormalTok{),}
                     \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{" "}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{,  }\StringTok{"2}\SpecialCharTok{\textbackslash{}n}\StringTok{(i.e., the population mean)"}\NormalTok{ ,}\FloatTok{2.5}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{breaks =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"The sampling distribution of the sample medians"}\NormalTok{, }\AttributeTok{subtitle =} \FunctionTok{expression}\NormalTok{(}\StringTok{"The sample medians are biased estimates of the mean for Poisson"}\SpecialCharTok{\textasciitilde{}}\NormalTok{(lambda}\SpecialCharTok{==}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=396px]{STA631-HW4-TylerReed_files/figure-latex/unnamed-chunk-7-1} \end{center}

Here we see that the median is a slightly biased estimate of lambda
because the mean of the sample medians is not equal to lambda.

If we have data that is Normally distributed then the median is an
unbiased estimate of the pupulation mean, however, it is more variable,
so the mean is better.

\begin{quote}
Now suppose that women are more likely than men to answer the survey,
with nonresponse depending only on sex. In that case, the sample will,
on average, overrepresent women, and women on average watch less
television than men; hence, the average number of hours watched in the
sample is now a \emph{biased} estimate of the proportion in the
population. It is possible to correct for this bias by reweighting the
sample as in Section 3.1; recognizing the existence of the bias is the
first step in fixing it. (p.~55, \emph{emphasis} in the original)
\end{quote}

Let' see if we can work this one out with a mini simulation, too.
Sticking with an overall population mean of 2 hours, let's presume women
watch 1.5 hours of television, on average, and men watch 2.5 hours, on
average. For simplicity, we'll further presume the overall population is
composed 50\%/50\% of men and women. The catch is we'll simulate out
samples such that they're 60\% women.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# total sample}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{mu\_women }\OtherTok{\textless{}{-}} \FloatTok{1.5}
\NormalTok{mu\_men   }\OtherTok{\textless{}{-}} \FloatTok{2.5}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}

\NormalTok{d }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{hours =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n }\SpecialCharTok{*} \FloatTok{0.6}\NormalTok{, }\AttributeTok{lambda =}\NormalTok{ mu\_women),}
                   \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n }\SpecialCharTok{*} \FloatTok{0.4}\NormalTok{, }\AttributeTok{lambda =}\NormalTok{ mu\_men)),}
         \AttributeTok{sex  =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"women"}\NormalTok{, }\StringTok{"men"}\NormalTok{), }\AttributeTok{times =}\NormalTok{ n }\SpecialCharTok{*} \FunctionTok{c}\NormalTok{(.}\DecValTok{6}\NormalTok{, .}\DecValTok{4}\NormalTok{)))}

\FunctionTok{glimpse}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,000
## Columns: 2
## $ hours <int> 2, 0, 1, 1, 3, 1, 2, 3, 4, 0, 2, 1, 0, 4, 1, 1, 4, 2, 4, 2, 2, 6~
## $ sex   <chr> "women", "women", "women", "women", "women", "women", "women", "~
\end{verbatim}

Here are the means, grouped by \texttt{sex}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sex) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(hours), }\AttributeTok{n=}\FunctionTok{n}\NormalTok{(), }\AttributeTok{var=}\FunctionTok{var}\NormalTok{(hours))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 4
##   sex    mean     n   var
##   <chr> <dbl> <int> <dbl>
## 1 men    2.36   400  2.17
## 2 women  1.47   600  1.55
\end{verbatim}

When divided by \texttt{sex}, the simulation appeared to produce
unbiased estimates of the subpopulation means. However, it is indeed
biased for the overall population.

Notice as well the variance here, since we have a Poisson RV the mean
and the variance are expected to be close. Why?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(hours))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##    mean
##   <dbl>
## 1  1.83
\end{verbatim}

See? It's a little low. Now we can use the weighting strategy from
Section 3.1 where we compute the weighted average following the formula

\[\text{weighted average} = \frac{\sum_j N_j \bar y_j}{\sum_j N_j},\]

where \(j\) indexes groups (\texttt{sex} in this example), \(\bar y_j\)
stands for the group-specific means, and \(N_j\) stands for the number
(or percent, in our example) of each group \emph{in the population}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(sex) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(hours)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{percent =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{weighted\_average =} \FunctionTok{sum}\NormalTok{(percent }\SpecialCharTok{*}\NormalTok{ mean) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(percent))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   weighted_average
##              <dbl>
## 1             1.92
\end{verbatim}

Happily, our weighted average now returns an unbiased estimate of the
population average for hours spent each day watching television.

\hypertarget{adjusting-inferences-to-account-for-bias-and-unmodeled-uncertainty.}{%
\subsubsection{4.3.2 Adjusting inferences to account for bias and
unmodeled
uncertainty.}\label{adjusting-inferences-to-account-for-bias-and-unmodeled-uncertainty.}}

\begin{quote}
How can we account for sources of error that are not in our statistical
model? In general, there are three ways to go: improve data collection,
expand the model, and increase stated uncertainty. (p.~56)
\end{quote}

\hypertarget{statistical-significance-hypothesis-testing-and-statistical-errors}{%
\subsection{4.4 Statistical significance, hypothesis testing, and
statistical
errors}\label{statistical-significance-hypothesis-testing-and-statistical-errors}}

\begin{quote}
One concern when performing data analysis is the possibility of
mistakenly coming to strong conclusions that do not replicate or do not
reflect real patterns in the underlying population. Statistical theories
of hypothesis testing and error analysis have been developed to quantify
these possibilities in the context of inference and decision making.
(p.~57)
\end{quote}

\hypertarget{statistical-significance.}{%
\subsubsection{4.4.1 Statistical
significance.}\label{statistical-significance.}}

\begin{quote}
Statistical significance is conventionally defined as a \(p\)-value less
than 0.05, relative to some \emph{null hypothesis} or prespecified value
that would indicate no effect present, as discussed below in the context
of hypothesis testing. For fitted regressions, this roughly corresponds
to coefficient estimates being labeled as statistically significant if
they are at least two standard errors from zero, or not statistically
significant otherwise. (p.~57, \emph{emphasis} in the original)
\end{quote}

The authors then considered a case where you flip a coin 20 times, with
eight of the trials coming up heads. The conventional null hypothesis
for a fail coin is that you'd have \(p = .5\) for heads or tails. Using
our skills from Section 4.2.3, we know the standard error for a
proportion is \(\sqrt{\hat p (1 - \hat p) / n}\).

\[H_o: p=.5\]

\[H_a: p \ne .5\] We do the experiment once and we get 8 successes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{20}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{8}

\CommentTok{\# the estimated probability}
\NormalTok{(p }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{/}\NormalTok{ n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the standard error}
\NormalTok{(se }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(p }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p) }\SpecialCharTok{/}\NormalTok{ n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1095445
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the 95\% CIs}
\NormalTok{p }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ se, }\DecValTok{2} \SpecialCharTok{*}\NormalTok{ se)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.180911 0.619089
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#a better method, use prop.test to get the Wilson intervals}
\FunctionTok{prop.test}\NormalTok{(y,n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1-sample proportions test with continuity correction
## 
## data:  y out of n, null probability 0.5
## X-squared = 0.45, df = 1, p-value = 0.5023
## alternative hypothesis: true p is not equal to 0.5
## 95 percent confidence interval:
##  0.1997709 0.6358833
## sample estimates:
##   p 
## 0.4
\end{verbatim}

Those confidence intervals clearly contain \(p = .5\) within their
bounds, leading us to conclude our results are not statistically
significantly different from the null hypothesis. We do not have
evidence in the data that the true proportion is different than 0.5.

An interpretation of a confidence interval is the values of a parameter
that would not be rejected using a 1-CL\% alpha level.

\hypertarget{hypothesis-testing-for-simple-comparisons.}{%
\subsubsection{4.4.2 Hypothesis testing for simple
comparisons.}\label{hypothesis-testing-for-simple-comparisons.}}

\begin{quote}
We shall review the key concepts of conventional hypothesis testing with
a simple hypothetical example. A randomized experiment is performed to
compare the effectiveness of two drugs for lowering cholesterol. The
mean and standard deviation of the post-treatment cholesterol levels are
\(\bar y_T\) and \(s_T\) for the \(n_T\) people in the treatment group,
and \(\bar y_C\) and \(s_C\) for the \(n_C\) people in the control
group. (p.~57)
\end{quote}

\hypertarget{estimate-standard-error-and-degrees-of-freedom.}{%
\paragraph{4.4.2.1 Estimate, standard error, and degrees of
freedom.}\label{estimate-standard-error-and-degrees-of-freedom.}}

\begin{quote}
The parameter of interest here is \(\theta = \mu_T - \mu_C\), the
expectation (population mean) of the post-test difference in cholesterol
between the two groups. Assuming the experiment has been done correctly,
the estimate is \$ \hat \theta= \bar y\_T - \bar y\_C\$ and the standard
error is \(\text{se} (\hat \theta) = \sqrt{s_C^2 / n_C + s_T^2 / n_T}\).
The approximate 95\% interval is then
\([\hat \theta \pm t_{n_C + n_T - 2}^{0.975} * \text{se} (\hat \theta)]\),
where \(t_{df}^{0.975}\) is the 97.5\textsuperscript{th} percentile of
the unit \(t\) distribution with \(df\) degrees of freedom. (p.~57)
\end{quote}

\hypertarget{null-and-alternative-hypotheses.}{%
\paragraph{4.4.2.2 Null and alternative
hypotheses.}\label{null-and-alternative-hypotheses.}}

\begin{quote}
To frame the above problem as a hypothesis test problem, one must define
\emph{null} and \emph{alternative} hypotheses. The null hypothesis is
\(\theta = 0\), that is, \(\theta_T = \theta_C\) , and the alternativeis
\(\theta \neq 0\), thatis, \(\theta_T \neq \theta_C\).

The hypothesis test is based on a \emph{test statistic} that summarizes
the deviation of the data from what would be expected under the null
hypothesis. The conventional test statistic in this sort of problem is
the absolute value of the \(t\)-score,
\(t = |\hat \theta| / \text{se}(\hat \theta)\), with the absolute value
representing a ``two-sided test,'' so called because either positive or
negative deviations from zero would be noteworthy. (p.~57,
\emph{emphasis} in the original)
\end{quote}

\hypertarget{p-value.}{%
\paragraph{\texorpdfstring{4.4.2.3
\(p\)-value.}{4.4.2.3 p-value.}}\label{p-value.}}

\begin{quote}
In a hypothesis test, the deviation of the data from the null hypothesis
is summarized by the \(p\)-\emph{value}, the probability of observing
something at least as extreme as the observed test statistic. For this
problem, under the null hypothesis the test statistic has a unit \(t\)
distribution with \(\nu\) degrees of freedom. (p.~57, \emph{emphasis} in
the original)
\end{quote}

If we let \texttt{theta\_hat} = \(\hat \theta\), \texttt{se\_theta} =
\(\text{se}(\hat \theta)\) \texttt{n\_C} = \(n_C\), and \texttt{n\_T} =
\(n_T\), we can use base \textbf{R} to compute our \(p\)-value like
this.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pt}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_hat) }\SpecialCharTok{/}\NormalTok{ se\_theta, }\AttributeTok{df =}\NormalTok{ n\_C }\SpecialCharTok{+}\NormalTok{ n\_T, }\AttributeTok{ncp =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{hypothesis-testing-general-formulation.}{%
\subsubsection{4.4.3 Hypothesis testing: general
formulation.}\label{hypothesis-testing-general-formulation.}}

\begin{quote}
In the simplest form of hypothesis testing, the null hypothesis \(H_0\)
represents a particular probability model, \(p(y)\), with potential
replication data \(y^\text{rep}\). To perform a hypothesis test, we must
define a test statistic \(T\), which is a function of the data. For any
given data \(y\), the \(p\)-value is then
\(\operatorname{Pr}(T(y^\text{rep}) \geq T(y))\): the probability of
observing, under the model, something as or more extreme than the data.
\end{quote}

Example:

Think of the model above where n=20 independent trials and we tested the
hypotheses,

\[H_o: p=.5\]

\[H_a: p \ne .5\] We do the experiment once and we get 8 successes. We
used a Normal approximation and the Wilson test to get a p-value, we
also could get the p-value by thinking about the probability model (this
is more exact), under the null hypotheses.

So our Null probability model is:

\[X \sim Binomial (n=20, p=0.5)\] Find, a p-value without using an
approximation, when our observed number of successes is 8.

What does it mean to be our observed value or more extreme?? Observed is
\$\hat p =8/20=.4 \$ What values of X are 8 or more extreme?
0,1,\ldots,8 then since it is two tailed 12,13,\ldots,20

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbinom}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{20}\NormalTok{,.}\DecValTok{5}\NormalTok{)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{pbinom}\NormalTok{(}\DecValTok{11}\NormalTok{,}\DecValTok{20}\NormalTok{,.}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5034447
\end{verbatim}

\begin{quote}
In regression modeling, testing is more complicated. The model to be fit
can be written as \(p(y|x, \theta)\), where \(\theta\) represents a set
of parameters including coefficients, residual standard deviation, and
possibly other parameters, and the null hypothesis might be that some
particular coefficient of interest equals zero. (p.~58)
\end{quote}

\hypertarget{comparisons-of-parameters-to-fixed-values-and-each-other-interpreting-confidence-intervals-as-hypothesis-tests.}{%
\subsubsection{4.4.4 Comparisons of parameters to fixed values and each
other: interpreting confidence intervals as hypothesis
tests.}\label{comparisons-of-parameters-to-fixed-values-and-each-other-interpreting-confidence-intervals-as-hypothesis-tests.}}

\begin{quote}
The hypothesis that a parameter equals zero (or any other fixed value)
can be directly tested by fitting the model that includes the parameter
in question and examining the corresponding 95\% interval. If the
interval excludes zero (or the specified fixed value), then the
hypothesis is said to be rejected at the 5\% level.

Testing whether two parameters are equal is equivalent to testing
whether their difference equals zero. We can do this by including both
parameters in the model and then examining the 95\% interval for their
difference. As with inference for a single parameter, the confidence
interval is commonly of more interest than the hypothesis test. (p.~58)
\end{quote}

\hypertarget{type-1-and-type-2-errors-and-why-we-dont-like-talking-about-them.}{%
\subsubsection{4.4.5 Type 1 and type 2 errors and why we don't like
talking about
them.}\label{type-1-and-type-2-errors-and-why-we-dont-like-talking-about-them.}}

\begin{quote}
Statistical tests are typically understood based on \emph{type 1
error}--the probability of falsely rejecting a null hypothesis, if it is
in fact true--and \emph{type 2 error}--the probability of not rejecting
a null hypothesis that is in fact false. But this paradigm does not
match up well with much of social science, or science more generally.
(pp.~58--59, \emph{emphasis} in the original)
\end{quote}

\hypertarget{type-m-magnitude-and-type-s-sign-errors.}{%
\subsubsection{4.4.6 Type M (magnitude) and type S (sign)
errors.}\label{type-m-magnitude-and-type-s-sign-errors.}}

\begin{quote}
A \emph{type S error} occurs when the sign of the estimated effect is of
the opposite direction as the true effect. A \emph{type M error} occurs
when the magnitude of the estimated effect is much different from the
true effect. A statistical procedure can be characterized by its type S
error rate--the probability of an estimate being of the opposite sign of
the true effect, conditional on the estimate being statistically
significant--and its expected exaggeration factor--the expected ratio of
the magnitude of the estimated effect divided by the magnitude of the
underlying effect. (p.~59, \emph{emphasis} in the original)
\end{quote}

For more on this, check out the
\href{https://CRAN.R-project.org/package=PRDA}{\textbf{PRDA} package},
which is designed to assess type S and M errors for a given study
design.

\hypertarget{hypothesis-testing-and-statistical-practice.}{%
\subsubsection{4.4.7 Hypothesis testing and statistical
practice.}\label{hypothesis-testing-and-statistical-practice.}}

\begin{quote}
We do not generally use null hypothesis significance testing in our own
work. In the fields in which we work, we do not generally think null
hypotheses can be true: in social science and public health, just about
every treatment one might consider will have \emph{some} effect, and no
comparisons or regression coefficient of interest will be \emph{exactly}
zero. We do not find it particularly helpful to formulate and test null
hypotheses that we know ahead of time cannot be true. Testing null
hypotheses is just a matter of data collection: with sufficient sample
size, any hypothesis can be rejected, and there is no real point to
gathering a mountain of data just to reject a hypothesis that we did not
believe in the first place. (p.~59, \emph{emphasis} in the original)
\end{quote}

\hypertarget{problems-with-the-concept-of-statistical-significance}{%
\subsection{4.5 Problems with the concept of statistical
significance}\label{problems-with-the-concept-of-statistical-significance}}

\begin{quote}
A common statistical error is to summarize comparisons by statistical
significance and to draw a sharp distinction between significant and
nonsignificant results. The approach of summarizing by statistical
significance has five pitfalls: two that are obvious and three that are
less well understood. (p.~60)
\end{quote}

\hypertarget{statistical-significance-is-not-the-same-as-practical-importance.}{%
\subsubsection{4.5.1 Statistical significance is not the same as
practical
importance.}\label{statistical-significance-is-not-the-same-as-practical-importance.}}

Small effects can be statistically significant, say a treatment results
in an increase in annual income of \$10 with a SE of \$2 in the USA.

\hypertarget{non-significance-is-not-the-same-as-zero.}{%
\subsubsection{4.5.2 Non-significance is not the same as
zero.}\label{non-significance-is-not-the-same-as-zero.}}

We are told to not accept the null hypothesis but it can be very
tempting to do just that.
\url{https://statmodeling.stat.columbia.edu/2020/09/17/we-want-certainty-even-when-its-not-appropriate/}
is a recent blog post that might be of interest.

\hypertarget{the-difference-between-significant-and-not-significant-is-not-itself-statistically-significant.}{%
\subsubsection{4.5.3 The difference between ``significant'' and ``not
significant'' is not itself statistically
significant.}\label{the-difference-between-significant-and-not-significant-is-not-itself-statistically-significant.}}

If you pay attention to the substantive literature, you'll likely find a
lot of examples of authors making this mistake.

I(Prof K) have made these mistakes in the past with how I wrote
something up. Xeffect is statistically diff, Yeffect is not\ldots{} then
talk about Xeffect, maybe think up some reason why Yeffect is different
than Xeffect\ldots{} oops\ldots. , then worst of all talk about the
difference between Data stories can be good, communication is important,
but it is very tricky to not over or under sell our results. There are
not hard and fast rules, you must have curiosity and ask a lot of
questions to understand your models.

\begin{quote}
Consider two independent studies with effect estimates and standard
errors of \(25 \pm 10\) and \(10 \pm 10\). The first study is
statistically significant at the 1\% level, and the second is not at all
significant at 1 standard error away from zero. Thus it would be
tempting to conclude that there is a large difference between the two
studies. In fact, however, the difference is not even close to being
statistically significant: the estimated difference is 15, with a
standard error of \(\sqrt{10^2 + 10^2} = 14\) (p.~61)
\end{quote}

\hypertarget{researcher-degrees-of-freedom-p-hacking-and-forking-paths.}{%
\subsubsection{\texorpdfstring{4.5.4 Researcher degrees of freedom,
\(p\)-hacking, and forking
paths.}{4.5.4 Researcher degrees of freedom, p-hacking, and forking paths.}}\label{researcher-degrees-of-freedom-p-hacking-and-forking-paths.}}

\begin{quote}
Another problem with statistical significance is that it can be attained
by multiple comparisons, or multiple potential comparisons. When there
are many ways that data can be selected, excluded, and analyzed in a
study, it is not difficult to attain a low \(p\)-value even in the
absence of any true underlying pattern. The problem here is \emph{not}
just the ``file-drawer effect'' of leaving non-significant findings
unpublished, but also that any given study can involve a large number of
``degrees of freedom'' available to the researcher when coding data,
deciding which variables to include in the analysis, and deciding how to
perform and summarize the statistical modeling. (p.~61, \emph{emphasis}
in the original)
\end{quote}

\hypertarget{the-statistical-significance-filter.}{%
\subsubsection{4.5.5 The statistical significance
filter.}\label{the-statistical-significance-filter.}}

A big and very important point:

\begin{quote}
A final concern is that statistically significant estimates tend to be
overestimates. This is the type M, or magnitude, error problem discussed
in Section 4.4. Any estimate with \(p < 0.05\) is by necessity at least
two standard errors from zero. If a study has a high noise level,
standard errors will be high, and so statistically significant estimates
will automatically be large, no matter how small the underlying effect.
Thus, routine reliance on published, statistically significant results
will lead to systematic overestimation of effect sizes and a distorted
view of the world. (p.~62)
\end{quote}

\hypertarget{example-a-flawed-study-of-ovulation-and-political-attitudes.}{%
\subsubsection{4.5.6 Example: A flawed study of ovulation and political
attitudes.}\label{example-a-flawed-study-of-ovulation-and-political-attitudes.}}

\emph{Note if time is short I might skip going over this example as it
is rather involved.}

Herein the authors discussed the flaws in the paper by Durante et al
(2013),
\href{https://journals.sagepub.com/doi/abs/10.1177/0956797612466416}{\emph{The
fluctuating female vote: Politics, religion, and the cvulatory cycle}}.

\hypertarget{moving-beyond-hypothesis-testing}{%
\subsection{4.7 Moving beyond hypothesis
testing}\label{moving-beyond-hypothesis-testing}}

\begin{quote}
Null hypothesis significance testing has all sorts of problems, but it
addresses a real concern in quantitative research: we want to be able to
make conclusions without being misled by noisy data, and hypothesis
testing provides a check on overinterpretation of noise. How can we get
this benefit of statistical reasoning while avoiding the overconfidence
and exaggerations that are associated with conventional reasoning based
on statistical significance? (p.~66)
\end{quote}

\begin{itemize}
\tightlist
\item
  analyze all your data
\item
  present all your comparisons
\item
  make your data and code public
\end{itemize}

\end{document}
